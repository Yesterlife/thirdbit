---
title: "Some Recent Papers I've Liked"
date: 2009-01-28 08:57:20
---
Cory J. Kapser and Michael W. Godfrey: "'Cloning considered harmful' considered harmful: patterns of cloning in software." <em>Empirical Software Engineering</em>, 2008, 13:645-692, DOI 10.1007/s10664-008-9076-6.

"...we have found significant evidence that cloning is often used in a variety of ways as a principled engineering tool."  Comes with a catalog of beneficial cloning design patterns.

<hr />Jeffrey C. Carver, Nachiappan Nagappan, and Alan Page: "The Impact of Educational Background on the Effectiveness of Requirements Inspections: An Empirical Study." <em>IEEE Transactions on Software Engineering</em>, 34(6), Nov/Dec 2008, DOI 10.1109/TSE.2008.49.

"...a large-scale controlled inspection experiment with over 70 professionals was conducted at Microsoft that focused on the relationship between an inspector's background and his effectiveness during a requirements inspection.  The results of the study showed that inspectors with university degrees in majors not related to computer science found significantly more defects than those with degrees in computer science majors. We also observed that level of education (Masters, PhD), prior industrial experience, or other job-related experiences did not signficantly impact the effectiveness of an inspector."

<hr />Lech Madeyski: "Impact of Pair Programming on Thoroughness and Fault Detection Effectiveness of Unit Test Suites." <em>Software Process Improvement and Practice</em>, 2008, 13:281-295, DOI 10.1002/spip.382.

"...we examined pair programming versus solo programming with respect to both thoroughness and fault detection effectiveness of test suites.  Branch coverage (BC) and mutation score indicator (MSI) were used as measures of how thoroughly tests exercise programs, and how effective they are, respectively. It turned out that the PP practice did not significant affect GC and MSU."

<hr />Victoria Stodden: "The Legal Framework for Reproducible Scientific Research: Licensing and Copyright." <em>Computing in Science &amp; Engineering</em>, Jan/Feb 2009.

"The author proposes the Reproducible Research Standard for all components of scientific researchers' scholarship, which should encourage replicable scientific investigation through attribution, the facilitation of greater collaboration, and the promotion of the engagement of the larger community..."

<hr />Khaled El Emam, Saida Benlarbi, Nishith Goel, and Shesh N. Rai: "The Confounding Effect of Class Size on the Validity of Object-Oriented Metrics." <em>IEEE Transasctions on Software Engineering</em>, 27(7), July 2001.

The abstract doesn't do it justice, so I'll summarize. Previous work to validate software metrics (cyclomatic complexity, coupling, etc.) has looked at correlations between metric values and things like post-release bug count. The authors repeated those experiments using bivariate analysis so that they could allocate a share of the blame to code size (measured by number of lines) and the metric in question. Turns out that code size accounted for all of the variation, i.e., the metrics didn't add have any actual predictive power once you normalized for the number of lines of code.
